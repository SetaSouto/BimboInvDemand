{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grupo Bimbo Inventory Demand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook con la implementación del modelo para [esta](https://www.kaggle.com/c/grupo-bimbo-inventory-demand) competencia en kaggle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas principales\n",
    "\n",
    "* Ocupar redes neuronales con la librería tensorflow.\n",
    "* Ocupar procesos guassianos con librería por seleccionar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Archivos\n",
    "\n",
    "* train.csv — the training set\n",
    "* test.csv — the test set\n",
    "* sample_submission.csv — a sample submission file in the correct format\n",
    "* cliente_tabla.csv — client names (can be joined with train/test on Cliente_ID)\n",
    "* producto_tabla.csv — product names (can be joined with train/test on Producto_ID)\n",
    "* town_state.csv — town and state (can be joined with train/test on Agencia_ID)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features\n",
    "\n",
    "* Semana — Week number (From Thursday to Wednesday)\n",
    "* Agencia_ID — Sales Depot ID\n",
    "* Canal_ID — Sales Channel ID\n",
    "* Ruta_SAK — Route ID (Several routes = Sales Depot)\n",
    "* Cliente_ID — Client ID\n",
    "* NombreCliente — Client name\n",
    "* Producto_ID — Product ID\n",
    "* NombreProducto — Product Name\n",
    "* Venta_uni_hoy — Sales unit this week (integer)\n",
    "* Venta_hoy — Sales this week (unit: pesos)\n",
    "* Dev_uni_proxima — Returns unit next week (integer)\n",
    "* Dev_proxima — Returns next week (unit: pesos)\n",
    "* Demanda_uni_equil — Adjusted Demand (integer) (This is the target you will predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero importamos las librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generamos una función para leer los datos. Retorna un dataFrame (pandas) de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fileName: Name of the csv file.\n",
    "# Columns: Array with the strings of each column in order.\n",
    "# rows: Number of rows to be read. None = All.\n",
    "def readData(fileName, columns, rows=None):\n",
    "    print('Reading file:', fileName)\n",
    "    start = time.time()\n",
    "    ret = pd.read_csv(fileName, usecols=columns, nrows=rows)\n",
    "    finish = time.time()\n",
    "    print('Shape:', str(ret.shape))\n",
    "    print('Execution time (seconds):', str(finish-start))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTA:** Importar una columna completa del archivo train demora aproximadamente 1 minuto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: Archivos/train.csv\n",
      "Shape: (1000000, 7)\n",
      "Execution time (seconds): 0.6487250328063965\n"
     ]
    }
   ],
   "source": [
    "# How many rows we'll load\n",
    "total_loaded = 1000000\n",
    "\n",
    "# Import the dataset\n",
    "columns = ['Semana', 'Agencia_ID', 'Canal_ID', 'Ruta_SAK', 'Cliente_ID', 'Producto_ID', 'Demanda_uni_equil']\n",
    "dataset = readData('Archivos/train.csv', columns, rows=total_loaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Inspeccionemos los datos primero antes de generar cualquier modelo.  \n",
    "Nuestro valor objetivo es la última columna de nuestros datos: *Demanda_uni_equil*, veamos como se distribuye sobre los datos que hemos tomado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Recieves an array of elements and plot its histogram\n",
    "# array: Values\n",
    "# maxValue: Max value to be shown in the histogram\n",
    "def toHist(array, maxValue, title='', xlabel='', ylabel=''):\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.hist(array[array<maxValue], bins=200, color='blue')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZwAAAEZCAYAAACjPJNSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4YRjZmZZOOGYmVkW/x9P5dcFkB7NwQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f320e04db70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "demanda_uni_equil = dataset['Demanda_uni_equil']\n",
    "toHist(demanda_uni_equil, 20, 'Histogram of Demanda_uni_equil', 'Value', 'Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Función para poder agrupar los valores de un dataFrame de acuerdo a los valores de cierta columna. Nos retorna una tupla con todos los valores distintos de la columna y un diccionario donde la llave es el valor y retorna un dataFrame con todas las filas con ese valor en la columna específica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Recieves a dataFrame and a column name (string) and returns a tuple \n",
    "# with the keys and a dict\n",
    "# where the key is de column value and the value is a dataFrame with all\n",
    "# the rows that have that column value.\n",
    "def groupBy(dataFrame, columnName):\n",
    "    different_values = dataFrame[columnName].drop_duplicates()\n",
    "    dictionary = dict()\n",
    "    for value in different_values:\n",
    "        # Select the data that have value in the columnName field.\n",
    "        data = dataFrame.loc[dataFrame[columnName]==value]\n",
    "        dictionary[value] = data\n",
    "    return (different_values, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función anterior se usaba, ya no."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Modelamiento con NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A priori, haremos que la red reciba como input las 7 características que entrega el archivo test. Ocuparemos la función de error del programa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lets define our accuracy\n",
    "def accuracy(predictions, real_outputs):\n",
    "    # It's the mean of our cost function across the predictions and results.\n",
    "    return np.sqrt( np.mean( np.square(np.log(predictions+1) - np.log(real_outputs+1)) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset (30000, 6)\n",
      "      outputs (30000, 1)\n",
      "Valid dataset (10000, 6)\n",
      "      outputs (10000, 1)\n",
      "Test  dataset (10000, 6)\n",
      "      outputs (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "# The data is divided (seudo)randomnly in 3 groups: train_dataset, valid_dataset, and test_dataset.\n",
    "\n",
    "train_size = 30000\n",
    "valid_size = 10000\n",
    "test_size = 10000\n",
    "\n",
    "#-------------------------------------------------------\n",
    "# TODO: Hacer que las características que no tienen un orden (todas menos semana) sean one hot encoding.\n",
    "#-------------------------------------------------------\n",
    "\n",
    "# 6 feautres + output\n",
    "train = dataset.sample(train_size)\n",
    "valid = dataset.sample(valid_size)\n",
    "test = dataset.sample(test_size)\n",
    "\n",
    "# 6 features only, now they are numpy arrays\n",
    "train_dataset = train.drop('Demanda_uni_equil', axis=1).as_matrix().astype('float32')\n",
    "valid_dataset = valid.drop('Demanda_uni_equil', axis=1).as_matrix().astype('float32')\n",
    "test_dataset = test.drop('Demanda_uni_equil', axis=1).as_matrix().astype('float32')\n",
    "\n",
    "# Outputs\n",
    "train_output = train['Demanda_uni_equil'].as_matrix().astype('float32').reshape((train_size, 1))\n",
    "valid_output = valid['Demanda_uni_equil'].as_matrix().astype('float32').reshape((valid_size, 1))\n",
    "test_output = test['Demanda_uni_equil'].as_matrix().astype('float32').reshape((test_size, 1))\n",
    "\n",
    "print('Train dataset', train_dataset.shape)\n",
    "print('      outputs', train_output.shape)\n",
    "print('Valid dataset', valid_dataset.shape)\n",
    "print('      outputs', valid_output.shape)\n",
    "print('Test  dataset', test_dataset.shape)\n",
    "print('      outputs', test_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nota\n",
    "Los modelos no estan funcionando bien, devuelven puros nan. Al parecer es la función de costo. La cambié y funcionó (devolvió números).  \n",
    "IDEAS:\n",
    "* Mala elección de la función loss.\n",
    "* Los input son demasiado grandes, hay overflow. Esto porque a veces muestra cuanto es el valor del loss y son número gigantescos. SOL: Encodear las variables muy grandes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The logistic model\n",
    "\n",
    "# With gradient descent training, even this much data is prohibitive.\n",
    "# Subset the training data for faster turnaround.\n",
    "train_subset = 10000\n",
    "\n",
    "features = 6\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    # Load the training, validation and test data into constants that are\n",
    "    # attached to the graph.\n",
    "    tf_train_dataset = tf.constant(train_dataset[:train_subset, :])\n",
    "    tf_train_output = tf.constant(train_output[:train_subset])\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "    # Variables.\n",
    "    # These are the parameters that we are going to be training. The weight\n",
    "    # matrix will be initialized using random valued following a (truncated)\n",
    "    # normal distribution. The biases get initialized to zero.\n",
    "    weights = tf.Variable(\n",
    "      tf.truncated_normal([features, 1]))\n",
    "    biases = tf.Variable(tf.zeros([1]))\n",
    "  \n",
    "    # Training computation.\n",
    "    \n",
    "    output = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.nn.l2_loss( output - tf_train_output)\n",
    "    #loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(output, tf_train_output))\n",
    "    #loss = tf.sqrt( tf.reduce_mean( tf.square(tf.log(output+1) - tf.log(tf_train_output+1)) ))\n",
    "    #loss = tf.reduce_mean( tf.log(output+1) - tf.log(tf_train_output+1) )\n",
    "\n",
    "    # Optimizer.\n",
    "    # We are going to find the minimum of this loss using gradient descent.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    # These are not part of training, but merely here so that we can report\n",
    "    # accuracy figures as we train.\n",
    "    train_prediction = output\n",
    "    valid_prediction = tf.matmul(tf_valid_dataset, weights) + biases\n",
    "    test_prediction = tf.matmul(tf_test_dataset, weights) + biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "------------------\n",
      "Loss at step 0: 1917697797390336.000000\n",
      "Training loss: 10.7914\n",
      "Validation loss: nan\n",
      "------------------\n",
      "Loss at step 100: nan\n",
      "Training loss: nan\n",
      "Validation loss: nan\n",
      "------------------\n",
      "Loss at step 200: nan\n",
      "Training loss: nan\n",
      "Validation loss: nan\n",
      "------------------\n",
      "Loss at step 300: nan\n",
      "Training loss: nan\n",
      "Validation loss: nan\n",
      "------------------\n",
      "Loss at step 400: nan\n",
      "Training loss: nan\n",
      "Validation loss: nan\n",
      "------------------\n",
      "Loss at step 500: nan\n",
      "Training loss: nan\n",
      "Validation loss: nan\n",
      "------------------\n",
      "Loss at step 600: nan\n",
      "Training loss: nan\n",
      "Validation loss: nan\n",
      "------------------\n",
      "Loss at step 700: nan\n",
      "Training loss: nan\n",
      "Validation loss: nan\n",
      "------------------\n",
      "Loss at step 800: nan\n",
      "Training loss: nan\n",
      "Validation loss: nan\n",
      "Test loss: nan\n"
     ]
    }
   ],
   "source": [
    "# Let's run it:\n",
    "\n",
    "num_steps = 801\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    # This is a one-time operation which ensures the parameters get initialized as\n",
    "    # we described in the graph: random weights for the matrix, zeros for the\n",
    "    # biases. \n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "        # and get the loss value and the training predictions returned as numpy\n",
    "        # arrays.\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "        \n",
    "        if (step % 100 == 0):\n",
    "            print('------------------')\n",
    "            print('Loss at step %d: %f' % (step, l))\n",
    "            print('Training loss:', accuracy(predictions, train_output[:train_subset, :]))\n",
    "            # Calling .eval() on valid_prediction is basically like calling run(), but\n",
    "            # just to get that one numpy array. Note that it recomputes all its graph\n",
    "            # dependencies.\n",
    "            print('Validation loss:', accuracy(\n",
    "                valid_prediction.eval(), valid_output))\n",
    "    print('Test loss:', accuracy(test_prediction.eval(), test_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo que no logro correr sin nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model deep neural network with stochastic gradient descent for speed.\n",
    "\n",
    "# How many features we want to use\n",
    "features = 6\n",
    "# Size of the placeholder in the train input data.\n",
    "batch_size = 512\n",
    "hidden_nodes = 1024\n",
    "beta = 0.001\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, features))\n",
    "    # We only have one output, the prediction.\n",
    "    tf_train_outputs = tf.placeholder(tf.float32, shape=(batch_size, 1))\n",
    "    # The valid dataset and the test dataset remain as constants.\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    #------------\n",
    "    # Variables:\n",
    "    #------------\n",
    "    \n",
    "    # First layer\n",
    "    weights1 = tf.Variable(\n",
    "        tf.truncated_normal([features, hidden_nodes]))\n",
    "    biases1 = tf.Variable(tf.zeros([hidden_nodes]))\n",
    "    \n",
    "    # Hidden layer\n",
    "    weights2 = tf.Variable(\n",
    "        tf.truncated_normal([hidden_nodes, 1]))\n",
    "    biases2 = tf.Variable(tf.zeros([1]))\n",
    "    \n",
    "    #----------------------\n",
    "    # Training computation.\n",
    "    #----------------------\n",
    "    \n",
    "    # Outputs after the first layer:\n",
    "    # We are using RELU, thats the non linearity in our model\n",
    "    outputs1 = tf.nn.relu( tf.matmul(tf_train_dataset, weights1) + biases1 )\n",
    "    \n",
    "    # Output after the second layer:\n",
    "    output = tf.matmul(outputs1, weights2) + biases2\n",
    "    \n",
    "    #-------\n",
    "    # Loss\n",
    "    #-------\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(output, tf_train_outputs))\n",
    "    #loss = tf.sqrt( tf.reduce_mean( tf.square(tf.log(output+1) - tf.log(tf_train_outputs+1)) )) #+beta*tf.l2_loss(weights1) + tf.nn.l2_loss(weights2)\n",
    "    \n",
    "    #-----------\n",
    "    # Optimizer\n",
    "    #-----------\n",
    "  \n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = output\n",
    "    valid_prediction = tf.matmul(\n",
    "        tf.nn.relu( tf.matmul(tf_valid_dataset, weights1) + biases1 ) , weights2) + biases2\n",
    "    test_prediction = tf.matmul(\n",
    "        tf.nn.relu( tf.matmul(tf_test_dataset, weights1) + biases1 ) , weights2) + biases2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 0.000000\n",
      "Minibatch accuracy: nan\n",
      "Validation accuracy: 46.8299\n",
      "Minibatch loss at step 100: nan\n",
      "Minibatch accuracy: nan\n",
      "Validation accuracy: nan\n",
      "Minibatch loss at step 200: nan\n",
      "Minibatch accuracy: nan\n",
      "Validation accuracy: nan\n",
      "Minibatch loss at step 300: nan\n",
      "Minibatch accuracy: nan\n",
      "Validation accuracy: nan\n",
      "Minibatch loss at step 400: nan\n",
      "Minibatch accuracy: nan\n",
      "Validation accuracy: nan\n",
      "Minibatch loss at step 500: nan\n",
      "Minibatch accuracy: nan\n",
      "Validation accuracy: nan\n",
      "Minibatch loss at step 600: nan\n",
      "Minibatch accuracy: nan\n",
      "Validation accuracy: nan\n",
      "Minibatch loss at step 700: nan\n",
      "Minibatch accuracy: nan\n",
      "Validation accuracy: nan\n",
      "Minibatch loss at step 800: nan\n",
      "Minibatch accuracy: nan\n",
      "Validation accuracy: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-5917215d3a94>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mtf_train_dataset\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf_train_outputs\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mbatch_outputs\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         _, l, predictions = session.run(\n\u001b[1;32m---> 20\u001b[1;33m           [optimizer, loss, train_prediction], feed_dict=feed_dict)\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Minibatch loss at step %d: %f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/fabian/anaconda3/envs/tensorflow/lib/python3.4/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    338\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 340\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    341\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/fabian/anaconda3/envs/tensorflow/lib/python3.4/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    562\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m       results = self._do_run(handle, target_list, unique_fetches,\n\u001b[1;32m--> 564\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    565\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    566\u001b[0m       \u001b[1;31m# The movers are no longer used. Delete them.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/fabian/anaconda3/envs/tensorflow/lib/python3.4/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    635\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m--> 637\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m    638\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    639\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/home/fabian/anaconda3/envs/tensorflow/lib/python3.4/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m    642\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    643\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 644\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    645\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatusNotOK\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    646\u001b[0m       \u001b[0merror_message\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_message\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/fabian/anaconda3/envs/tensorflow/lib/python3.4/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m    626\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m--> 628\u001b[1;33m             session, None, feed_dict, fetch_list, target_list, None)\n\u001b[0m\u001b[0;32m    629\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    630\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run the model\n",
    "\n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_output.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_outputs = train_output[offset:(offset + batch_size),:]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_outputs : batch_outputs}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 100 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print('Minibatch accuracy:', accuracy(predictions, batch_outputs))\n",
    "            print('Validation accuracy:', accuracy(\n",
    "                valid_prediction.eval(), valid_output))\n",
    "    print('Test accuracy:', accuracy(test_prediction.eval(), test_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
